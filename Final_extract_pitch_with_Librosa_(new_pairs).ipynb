{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DffgjUBSnueH"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from moviepy.editor import VideoFileClip\n",
        "from google.colab import files\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcR_xJonAQXZ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQLdWNKldaEa",
        "outputId": "8986bd7e-2325-48f3-aa72-9bb94531d396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "TUDa5CKcipjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmEnDouyday7"
      },
      "outputs": [],
      "source": [
        "def get_frame_rate(file_path):\n",
        "    video_clip = VideoFileClip(file_path)\n",
        "\n",
        "    global frame_rate\n",
        "\n",
        "    frame_rate = video_clip.fps\n",
        "    video_clip.close()\n",
        "\n",
        "    return frame_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFElmzvQoAOL"
      },
      "outputs": [],
      "source": [
        "def timestamp_to_seconds(timestamp,frame_rate):\n",
        "    hours, minutes, seconds, frames = map(int, timestamp.split(':'))\n",
        "    return (hours * 3600 + minutes * 60 + seconds) + frames / frame_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWCnpZ_voAWW"
      },
      "outputs": [],
      "source": [
        "def duration_cal(row, frame_rate):\n",
        "    start_seconds = timestamp_to_seconds(row['Start.Timecode'], frame_rate)\n",
        "    end_seconds = timestamp_to_seconds(row['End.Timecode'], frame_rate)\n",
        "\n",
        "    duration_seconds = end_seconds - start_seconds\n",
        "\n",
        "    return duration_seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path info to get the transcription\n",
        "mypath = '/content/drive/My Drive/closeness/Observation_Study_Segments'\n",
        "\n",
        "def get_data(pair_num,participant_num,question_num):\n",
        "  transcription_path = os.path.join(mypath, f'Pair {pair_num}', f'Pair{pair_num}_Transcriptions.xlsx')\n",
        "\n",
        "  sheet_name = f'Participant{participant_num}_Q{question_num}'\n",
        "\n",
        "  # read transcription\n",
        "  ts_all = pd.read_excel(transcription_path, sheet_name=sheet_name, header=None)\n",
        "  ts_all.columns = ['Start.Timecode', 'End.Timecode', 'Speaker', 'Transcript']\n",
        "  ts_all['Speaker'] = ts_all['Speaker'].str.replace(' ', '')\n",
        "\n",
        "  return ts_all"
      ],
      "metadata": {
        "id": "dWCaBOd6lTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAqc8kOQoAYt"
      },
      "outputs": [],
      "source": [
        "def speaker(pair_num,participant_num,question_num):\n",
        "  #file_path = mypath + '/Pair' + pair_num + '/Pair' + pair_num + '_' + participant_num + '.mp4'\n",
        "  file_path1 = os.path.join(mypath, f'Pair {pair_num}', f'Pair{pair_num}_{participant_num}.mp4')\n",
        "  file_path2 = os.path.join(mypath, f'Pair {pair_num}', f'Pair{pair_num} {participant_num}.mp4')\n",
        "\n",
        "  if os.path.exists(file_path1):\n",
        "    file_path = file_path1\n",
        "  else:\n",
        "    file_path = file_path2\n",
        "\n",
        "  frame_rate = get_frame_rate(file_path)\n",
        "\n",
        "  ts_all = get_data(pair_num,participant_num,question_num)\n",
        "  speak1_ts = ts_all[ts_all['Speaker'] == 'Speaker1']\n",
        "  speak2_ts = ts_all[ts_all['Speaker'] == 'Speaker2']\n",
        "\n",
        "  speak1_ts['duration'] = speak1_ts.apply(duration_cal, axis=1, frame_rate=frame_rate)\n",
        "  speak2_ts['duration'] = speak2_ts.apply(duration_cal, axis=1, frame_rate=frame_rate)\n",
        "\n",
        "  return file_path,speak1_ts,speak2_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVyUy_EyoAa0"
      },
      "outputs": [],
      "source": [
        "def process_combined(df, file_path):\n",
        "    # Initialize arrays to store combined audio and durations\n",
        "    combined_audio = np.array([])\n",
        "    combined_durations = []\n",
        "\n",
        "    # Iterate over each row in the dataframe\n",
        "    for index, row in df.iterrows():\n",
        "        start_time = timestamp_to_seconds(row['Start.Timecode'],frame_rate)\n",
        "        duration = row['duration']\n",
        "\n",
        "        # Load audio segment\n",
        "        audio_segment, sr = librosa.load(file_path, offset=start_time, duration=duration)\n",
        "\n",
        "        # Concatenate audio segment and update combined durations\n",
        "        combined_audio = np.concatenate([combined_audio, audio_segment])\n",
        "        combined_durations.append(duration)\n",
        "\n",
        "    return combined_audio,sr,combined_durations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seconds_to_timestamp(seconds):\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    seconds = int(seconds % 60)\n",
        "    frames = int((seconds % 1) * frame_rate)\n",
        "    return f\"{hours:02}:{minutes:02}:{seconds:02}:{frames:02}\""
      ],
      "metadata": {
        "id": "2vfuYeoAsGZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update - based on timestamp (result: each second has lots of pitch result)\n",
        "def feature_extraction(combined_audio, sr, frame_rate):\n",
        "    pitches, magnitudes = librosa.core.piptrack(y=combined_audio, sr=sr)\n",
        "\n",
        "    pitch_values = []\n",
        "    timestamps = []\n",
        "\n",
        "    for t in range(pitches.shape[1]):\n",
        "        pitch = pitches[:, t]\n",
        "        magnitude = magnitudes[:, t]\n",
        "\n",
        "        if np.any(magnitude > 0):\n",
        "            index = magnitude.argmax()\n",
        "            pitch_value = pitch[index]\n",
        "        else:\n",
        "            pitch_value = 0\n",
        "\n",
        "        timestamp = t / frame_rate\n",
        "        pitch_values.append(pitch_value)\n",
        "        timestamps.append(timestamp)\n",
        "\n",
        "    result = list(zip(timestamps, pitch_values))\n",
        "\n",
        "    #variation_combined = np.var(combined_audio)\n",
        "\n",
        "    return result#, variation_combined"
      ],
      "metadata": {
        "id": "RCj9CzjKpGP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pitch_with_timestamp(pitch):\n",
        "  df = pd.DataFrame(pitch, columns=['Timestamp', 'Pitch'])\n",
        "  df['Timestamp'] = df['Timestamp'].apply(lambda x: seconds_to_timestamp(x))\n",
        "\n",
        "  grouped_df = df.groupby('Timestamp')['Pitch'].mean().reset_index()\n",
        "  return grouped_df"
      ],
      "metadata": {
        "id": "LvQAehZC_u-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summary_run(pair_num,participant_num):\n",
        "  file_path, speak1_ts, speak2_ts = speaker(pair_num,participant_num,question_num)\n",
        "\n",
        "  # load the audio\n",
        "  combined_audio_1, sr, combined_durations_1 = process_combined(speak1_ts, file_path)\n",
        "  combined_audio_2, sr, combined_durations_2 = process_combined(speak2_ts, file_path)\n",
        "\n",
        "  # feature extraction\n",
        "  pitch_1 = feature_extraction(combined_audio_1, sr,frame_rate)\n",
        "  pitch_2 = feature_extraction(combined_audio_2, sr,frame_rate)\n",
        "\n",
        "  # add timestamp\n",
        "  pitch_1_df = pitch_with_timestamp(pitch_1)\n",
        "  pitch_2_df = pitch_with_timestamp(pitch_2)\n",
        "\n",
        "  return pitch_1_df,pitch_2_df#,variation_1,variation_2"
      ],
      "metadata": {
        "id": "uYPLgFEsJija"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_output(pair_num, participant_num, pitch_1_df, pitch_2_df):\n",
        "  output_folder = '/content/drive/My Drive/closeness/csv_output/July/'\n",
        "\n",
        "  file_path_1 = os.path.join(output_folder, f\"pair{pair_num}_participant{participant_num}_speaker1_pitch.csv\")\n",
        "  file_path_2 = os.path.join(output_folder, f\"pair{pair_num}_participant{participant_num}_speaker2_pitch.csv\")\n",
        "\n",
        "  pitch_1_df.to_csv(file_path_1, index=False)\n",
        "  pitch_2_df.to_csv(file_path_2, index=False)"
      ],
      "metadata": {
        "id": "4SwYEBrFJ9nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run**"
      ],
      "metadata": {
        "id": "S3FAoMVokzKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair 11 Participant 1_Q3 only has Speaker2\n",
        "\n",
        "**Pair 12 Participant 1_Q3 has 2 rows of data without specifying Speaker 1 or Speaker 2 (skipped those 2 rows)\n",
        "\n",
        "**Pair 20 doesn't have transcription data\n",
        "\n",
        "**Pair 31 doesn't have transcription data\n",
        "\n",
        "**Pair 33 doesn't have transcription data"
      ],
      "metadata": {
        "id": "-6TeHTHd5uwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only need to update here\n",
        "pair_num = '29'\n",
        "participant_num = '2'\n",
        "question_num = '8'"
      ],
      "metadata": {
        "id": "_4jrGLy6b2hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitch_1_df,pitch_2_df = summary_run(pair_num,participant_num)"
      ],
      "metadata": {
        "id": "87GBIDR-TcQu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_output(pair_num,participant_num, pitch_1_df, pitch_2_df)"
      ],
      "metadata": {
        "id": "IonFRB51TjEm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TUDa5CKcipjV"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}